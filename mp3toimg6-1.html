<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Spectrogram Player (With Notation)</title>
  <style>
    body { background: #111; color: #fff; font-family: sans-serif; margin: 0; text-align: center; }
    .controls { padding: 10px; }
    .controls > * { margin: 6px; font-size: 16px; }
    #timeStatus { margin-top: 6px; font-size: 14px; font-style: italic; }
    .canvasRowContainer { position: relative; display: inline-block; margin-bottom: 20px; }
    canvas.spectrogramCanvas { background: #000; display: block; }
    canvas.cursorOverlayCanvas { position: absolute; left: 0; top: 0; pointer-events: auto; }
    canvas.annotationCanvas { position: absolute; left: 0; pointer-events: auto; }
    #playerControls { margin: 10px; display: none; }
    #playerControls button { margin: 0 6px; font-size: 16px; padding: 6px 12px; }
    #annotationControls { display: none; margin: 10px; }
    #annotationControls label { margin-right: 10px; }
  </style>
</head>
<body>

  <h1>Spectrogram Player (With Notation)</h1>

  <div class="controls">
    <input type="file" id="fileInput" accept=".mp3">
    <label>Speed:
      <select id="playbackSpeedSelect">
        <option>1</option><option>2</option><option>4</option>
        <option>30</option><option>60</option><option>120</option>
      </select> ×
    </label>
    <button id="startGenerateButton">Play (Phase 1)</button>
    <button id="pauseGenerateButton" disabled>Pause</button>
    <button id="exportButton">Download Spectrogram</button>
    <div id="timeStatus">No file loaded.</div>
    <div id="fileDurationStatus"></div>
  </div>

  <div id="playerControls">
    <button id="rewind15Seconds">◀ 15 s</button>
    <button id="playPauseButton">Play/Pause</button>
    <button id="forward15Seconds">15 s ▶</button>
  </div>

  <div id="annotationControls">
    <label>
      Mode:
      <select id="interactionMode">
        <option value="seek">Playback</option>
        <option value="annotate">Annotation</option>
      </select>
    </label>
    <label>
      Color:
      <input type="color" id="annotationColor" value="#00ff00">
    </label>
    <button id="eraserButton">Eraser</button>
  </div>

  <div id="rowsContainer"></div>

  <script>
  // Constants for layout dimensions and visual spacing
  const SCREEN_WIDTH = window.innerWidth;
  const ROW_HEIGHT = 300;
  const MINUTE_TICK_HEIGHT = 10;
  const LABEL_AREA_HEIGHT = 20;
  const ANNOTATION_STRIP_HEIGHT = 20;
  const USEFUL_CANVAS_HEIGHT = ROW_HEIGHT - MINUTE_TICK_HEIGHT - LABEL_AREA_HEIGHT - ANNOTATION_STRIP_HEIGHT;
  const PIXEL_LABEL_INTERVAL = 100;
  const LABEL_PADDING = 6;

  // References to important DOM elements (UI controls and containers)
  const fileInputElement = document.getElementById('fileInput');
  const playbackSpeedSelectElement = document.getElementById('playbackSpeedSelect');
  const startGenerateButtonElement = document.getElementById('startGenerateButton');
  const pauseGenerateButtonElement = document.getElementById('pauseGenerateButton');
  const exportButtonElement = document.getElementById('exportButton');
  const timeStatusElement = document.getElementById('timeStatus');
  const fileDurationStatusElement = document.getElementById('fileDurationStatus');
  const rowsContainerElement = document.getElementById('rowsContainer');
  const playerControlsElement = document.getElementById('playerControls');
  const rewind15SecondsButton = document.getElementById('rewind15Seconds');
  const playPauseButtonElement = document.getElementById('playPauseButton');
  const forward15SecondsButton = document.getElementById('forward15Seconds');
  const annotationControlsElement = document.getElementById('annotationControls');
  const interactionModeSelectElement = document.getElementById('interactionMode');
  const annotationColorInputElement = document.getElementById('annotationColor');
  const eraserButtonElement = document.getElementById('eraserButton');

  // Audio and rendering state
  let generatePhaseAudioElement = null; // For Phase 1 playback and generation
  let audioAnalyzerNode = null;         // Used to analyze the audio data
  let audioFrequencyDataArray = null;   // Stores frequency bin values
  let drawColumnPosition = 0;           // Current X drawing position
  let finalSpectrogramRowWidth = 0;     // Final width of last row (can be partial)
  let hasSpectrogramCompleted = false;
  let isGeneratePhasePaused = false;
  let animationFrameRequestId = null;
  let audioContext = null;              // Web Audio API context

  // Arrays to hold rendering canvases and time mapping
  const spectrogramCanvasRows = [];
  const cursorOverlayCanvasRows = [];
  const rowStartTimeStamps = [];
  const annotationCanvasRows = [];

  // Playback phase state
  let playbackPhaseAudioElement = null;
  let isPlaybackPhasePlaying = false;
  let currentAnnotationColor = '#00ff00';
  let currentInteractionMode = 'seek';

  // Interaction flags
  let isMouseDragging = false;
  let isTouchDragging = false;

  // Converts a time value (in seconds) to a readable "1h2m3s" string
  function formatTime(seconds) {
    const hours = Math.floor(seconds / 3600);
    const minutes = Math.floor((seconds % 3600) / 60);
    const secs = Math.floor(seconds % 60);
    return `${hours}h${minutes}m${secs}s`;
  }

  // Updates the time display for Phase 1
  function updateTimeStatus() {
    if (!generatePhaseAudioElement) {
      timeStatusElement.textContent = 'No file loaded.';
      return;
    }
    const currentTime = generatePhaseAudioElement.currentTime;
    const totalDuration = generatePhaseAudioElement.duration || 0;
    timeStatusElement.textContent = `Processing: ${formatTime(currentTime)} / ${formatTime(totalDuration)}`;
  }

  // Creates a new row (horizontal strip) for spectrogram rendering
  function createSpectrogramRow() {
    const rowContainerDiv = document.createElement('div');
    rowContainerDiv.className = 'canvasRowContainer';

    // Create and initialize the spectrogram canvas (shows color frequencies)
    const spectrogramCanvas = document.createElement('canvas');
    spectrogramCanvas.width = SCREEN_WIDTH;
    spectrogramCanvas.height = ROW_HEIGHT;
    spectrogramCanvas.className = 'spectrogramCanvas';
    const spectrogramCanvasContext = spectrogramCanvas.getContext('2d');
    spectrogramCanvasContext.fillStyle = '#000';
    spectrogramCanvasContext.fillRect(0, 0, SCREEN_WIDTH, ROW_HEIGHT);
    rowContainerDiv.appendChild(spectrogramCanvas);
    spectrogramCanvasRows.push({ canvas: spectrogramCanvas, canvasRenderingContext: spectrogramCanvasContext });

    // Create the transparent overlay canvas to show the red playback cursor
    const cursorOverlayCanvas = document.createElement('canvas');
    cursorOverlayCanvas.width = SCREEN_WIDTH;
    cursorOverlayCanvas.height = ROW_HEIGHT;
    cursorOverlayCanvas.className = 'cursorOverlayCanvas';
    const cursorOverlayContext = cursorOverlayCanvas.getContext('2d');
    rowContainerDiv.appendChild(cursorOverlayCanvas);
    cursorOverlayCanvasRows.push({ canvas: cursorOverlayCanvas, canvasRenderingContext: cursorOverlayContext });

    // Create the annotation strip canvas for this row
    const annotationCanvas = document.createElement('canvas');
    annotationCanvas.width = SCREEN_WIDTH;
    annotationCanvas.height = ANNOTATION_STRIP_HEIGHT;
    annotationCanvas.className = 'annotationCanvas';
    annotationCanvas.style.position = 'absolute';
    annotationCanvas.style.top = `${USEFUL_CANVAS_HEIGHT + MINUTE_TICK_HEIGHT + LABEL_AREA_HEIGHT}px`;
    annotationCanvas.style.left = '0';
    annotationCanvas.style.pointerEvents = 'auto';
    const annotationCanvasContext = annotationCanvas.getContext('2d');
    annotationCanvasRows.push({ canvas: annotationCanvas, canvasRenderingContext: annotationCanvasContext });
    rowContainerDiv.appendChild(annotationCanvas);

    // Record the time this row starts
    rowStartTimeStamps.push(generatePhaseAudioElement.currentTime);
    rowsContainerElement.appendChild(rowContainerDiv);

    drawColumnPosition = 0; // Reset draw position for new row
  }

  // Draws the spectrogram frame-by-frame during Phase 1
  function drawSpectrogramFrame() {
    if (generatePhaseAudioElement.paused || isGeneratePhasePaused) return;

    audioAnalyzerNode.getByteFrequencyData(audioFrequencyDataArray);

    // Create a new row if we've filled the current one
    if (drawColumnPosition >= SCREEN_WIDTH) {
      createSpectrogramRow();
    }

    // Get the current row and draw one vertical slice based on frequency data
    const currentSpectrogramRow = spectrogramCanvasRows[spectrogramCanvasRows.length - 1];
    const canvasRenderingContext = currentSpectrogramRow.canvasRenderingContext;

    for (let i = 0; i < audioAnalyzerNode.frequencyBinCount; i++) {
      const frequencyValue = audioFrequencyDataArray[i];
      canvasRenderingContext.fillStyle = `hsl(${frequencyValue * 1.5},100%,50%)`;
      canvasRenderingContext.fillRect(drawColumnPosition, USEFUL_CANVAS_HEIGHT - i, 1, 1);
    }

    // Draw time markers (vertical ticks and time labels)
    const elapsedWholeSeconds = Math.floor(generatePhaseAudioElement.currentTime);
    if (elapsedWholeSeconds % 60 === 0) {
      canvasRenderingContext.strokeStyle = '#fff';
      canvasRenderingContext.lineWidth = (elapsedWholeSeconds % 900 === 0 ? 24 : 8);
      canvasRenderingContext.beginPath();
      canvasRenderingContext.moveTo(drawColumnPosition + 0.5, USEFUL_CANVAS_HEIGHT);
      canvasRenderingContext.lineTo(drawColumnPosition + 0.5, USEFUL_CANVAS_HEIGHT + MINUTE_TICK_HEIGHT);
      canvasRenderingContext.stroke();
    }

    if (drawColumnPosition % PIXEL_LABEL_INTERVAL === 0) {
      const label = formatTime(generatePhaseAudioElement.currentTime);
      const textWidth = canvasRenderingContext.measureText(label).width;
      canvasRenderingContext.fillStyle = '#000';
      canvasRenderingContext.fillRect(drawColumnPosition - textWidth / 2 - LABEL_PADDING, USEFUL_CANVAS_HEIGHT + MINUTE_TICK_HEIGHT, textWidth + 2 * LABEL_PADDING, LABEL_AREA_HEIGHT);
      canvasRenderingContext.fillStyle = '#fff';
      canvasRenderingContext.textAlign = 'center';
      canvasRenderingContext.textBaseline = 'top';
      canvasRenderingContext.font = '12px sans-serif';
      canvasRenderingContext.fillText(label, drawColumnPosition, USEFUL_CANVAS_HEIGHT + MINUTE_TICK_HEIGHT + 2);
    }

    drawColumnPosition++;
    updateTimeStatus();
    animationFrameRequestId = requestAnimationFrame(drawSpectrogramFrame);

    // Detect when spectrogram phase is finished
    const FINISH_MARGIN_SECONDS = 0.05;
    if (!hasSpectrogramCompleted &&
        generatePhaseAudioElement.currentTime >= generatePhaseAudioElement.duration - FINISH_MARGIN_SECONDS) {
      handleSpectrogramCompletion();
      return;
    }
  }


  // Called when the spectrogram generation phase completes
  function handleSpectrogramCompletion() {
    hasSpectrogramCompleted = true;

    // Stop any ongoing drawing animation loop
    if (animationFrameRequestId) {
      cancelAnimationFrame(animationFrameRequestId);
      animationFrameRequestId = null;
    }

    // Enable playback controls and initialize the playback phase audio element
    setupPlaybackPhaseAudio();
  }

  // Sets up the audio element and UI for Phase 2 playback (with seek and annotation)
  function setupPlaybackPhaseAudio() {
    // Create a new audio element for playback phase
    playbackPhaseAudioElement = new Audio(generatePhaseAudioElement.src);

    // Show playback controls and hide generation controls
    playerControlsElement.style.display = 'block';
    startGenerateButtonElement.style.display = 'none';
    pauseGenerateButtonElement.style.display = 'none';

    // Add event listeners to playback controls
    playPauseButtonElement.addEventListener('click', togglePlayPausePlaybackPhase);
    rewind15SecondsButton.addEventListener('click', rewindPlayback15Seconds);
    forward15SecondsButton.addEventListener('click', forwardPlayback15Seconds);

    // Setup cursor update during playback
    playbackPhaseAudioElement.addEventListener('timeupdate', updatePlaybackCursor);

    // Setup annotation interaction event listeners on annotation canvases
    annotationCanvasRows.forEach(({ canvas }) => {
      canvas.addEventListener('mousedown', startAnnotation);
      canvas.addEventListener('mousemove', continueAnnotation);
      canvas.addEventListener('mouseup', endAnnotation);
      canvas.addEventListener('mouseleave', endAnnotation);
      canvas.addEventListener('touchstart', startAnnotation, { passive: true });
      canvas.addEventListener('touchmove', continueAnnotation, { passive: true });
      canvas.addEventListener('touchend', endAnnotation);
    });

    // Interaction mode change listener
    interactionModeSelectElement.addEventListener('change', handleInteractionModeChange);

    // Eraser button sets color to transparent (erase)
    eraserButtonElement.addEventListener('click', () => {
      currentAnnotationColor = 'rgba(0,0,0,0)'; // Transparent to erase
      annotationColorInputElement.value = '#000000'; // Reset color picker UI
      currentInteractionMode = 'annotate';
      interactionModeSelectElement.value = 'annotate';
    });

    // Color picker change listener
    annotationColorInputElement.addEventListener('input', (event) => {
      currentAnnotationColor = event.target.value;
      currentInteractionMode = 'annotate';
      interactionModeSelectElement.value = 'annotate';
    });

    // Initialize playback as paused
    isPlaybackPhasePlaying = false;
  }

  // Toggles between play and pause for playback phase
  function togglePlayPausePlaybackPhase() {
    if (!playbackPhaseAudioElement) return;

    if (isPlaybackPhasePlaying) {
      playbackPhaseAudioElement.pause();
      playPauseButtonElement.textContent = 'Play';
      isPlaybackPhasePlaying = false;
    } else {
      playbackPhaseAudioElement.play();
      playPauseButtonElement.textContent = 'Pause';
      isPlaybackPhasePlaying = true;
    }
  }

  // Rewind playback by 15 seconds
  function rewindPlayback15Seconds() {
    if (!playbackPhaseAudioElement) return;
    playbackPhaseAudioElement.currentTime = Math.max(0, playbackPhaseAudioElement.currentTime - 15);
  }

  // Forward playback by 15 seconds
  function forwardPlayback15Seconds() {
    if (!playbackPhaseAudioElement) return;
    playbackPhaseAudioElement.currentTime = Math.min(playbackPhaseAudioElement.duration, playbackPhaseAudioElement.currentTime + 15);
  }

  // Updates the red vertical playback cursor on each spectrogram row according to playback time
  function updatePlaybackCursor() {
    if (!playbackPhaseAudioElement) return;

    // Clear all cursor overlay canvases first
    cursorOverlayCanvasRows.forEach(({ canvasRenderingContext }) => {
      canvasRenderingContext.clearRect(0, 0, SCREEN_WIDTH, ROW_HEIGHT);
    });

    // Find which row corresponds to current playback time and draw cursor
    for (let rowIndex = 0; rowIndex < rowStartTimeStamps.length; rowIndex++) {
      const rowStartTime = rowStartTimeStamps[rowIndex];
      const nextRowStartTime = rowStartTimeStamps[rowIndex + 1] || playbackPhaseAudioElement.duration;

      if (playbackPhaseAudioElement.currentTime >= rowStartTime && playbackPhaseAudioElement.currentTime < nextRowStartTime) {
        const elapsedTimeInRow = playbackPhaseAudioElement.currentTime - rowStartTime;
        // Calculate X position of cursor based on elapsed time and pixel rate (assume 1 second = 100px or adjust accordingly)
        const pixelsPerSecond = SCREEN_WIDTH / (nextRowStartTime - rowStartTime);
        const cursorXPosition = elapsedTimeInRow * pixelsPerSecond;

        const cursorCanvasContext = cursorOverlayCanvasRows[rowIndex].canvasRenderingContext;
        cursorCanvasContext.strokeStyle = 'red';
        cursorCanvasContext.lineWidth = 2;
        cursorCanvasContext.beginPath();
        cursorCanvasContext.moveTo(cursorXPosition + 0.5, 0);
        cursorCanvasContext.lineTo(cursorXPosition + 0.5, ROW_HEIGHT);
        cursorCanvasContext.stroke();
      }
    }
  }

  // Handles user changing interaction mode (seek vs annotate)
  function handleInteractionModeChange(event) {
    currentInteractionMode = event.target.value;
  }

  // Begin annotation on mouse or touch down
  function startAnnotation(event) {
    if (currentInteractionMode !== 'annotate') return;

    isMouseDragging = true;
    applyAnnotation(event);
  }

  // Continue annotation as user moves mouse or finger
  function continueAnnotation(event) {
    if (!isMouseDragging) return;
    applyAnnotation(event);
  }

  // End annotation when mouse or touch is released or leaves canvas
  function endAnnotation(event) {
    if (isMouseDragging) {
      isMouseDragging = false;
    }
  }

  // Applies annotation color to the segment user clicked or dragged over
  function applyAnnotation(event) {
    event.preventDefault();

    // Identify which annotation canvas the event is targeting
    const targetCanvas = event.target;
    if (!annotationCanvasRows.some(({ canvas }) => canvas === targetCanvas)) return;

    const annotationCanvasIndex = annotationCanvasRows.findIndex(({ canvas }) => canvas === targetCanvas);
    const annotationCanvasContext = annotationCanvasRows[annotationCanvasIndex].canvasRenderingContext;

    // Calculate mouse or touch X coordinate relative to the canvas
    let clientX;
    if (event.touches && event.touches.length > 0) {
      clientX = event.touches[0].clientX;
    } else {
      clientX = event.clientX;
    }

    const boundingRect = targetCanvas.getBoundingClientRect();
    const relativeX = clientX - boundingRect.left;

    // Quantize X position to 50px segments (coarse resolution)
    const segmentWidth = 50;
    const segmentStartX = Math.floor(relativeX / segmentWidth) * segmentWidth;

    // Apply the selected color to the 50px-wide segment
    annotationCanvasContext.fillStyle = currentAnnotationColor;
    annotationCanvasContext.fillRect(segmentStartX, 0, segmentWidth, ANNOTATION_STRIP_HEIGHT);
  }

  // Initialization: Set up file input event to load audio and start Phase 1
  fileInputElement.addEventListener('change', (event) => {
    const file = event.target.files[0];
    if (!file) return;

    // Create URL for audio file and initialize audio element for Phase 1 generation
    if (generatePhaseAudioElement) {
      generatePhaseAudioElement.pause();
      generatePhaseAudioElement.src = '';
    }

    generatePhaseAudioElement = new Audio(URL.createObjectURL(file));
    generatePhaseAudioElement.crossOrigin = 'anonymous';

    generatePhaseAudioElement.addEventListener('canplay', () => {
      fileDurationStatusElement.textContent = `Duration: ${formatTime(generatePhaseAudioElement.duration)}`;
      setupAudioContextAndStartGeneration();
    });
  });

  // Creates Web Audio API context, analyzer node, and starts spectrogram drawing loop
  function setupAudioContextAndStartGeneration() {
    if (audioContext) {
      audioContext.close();
    }
    audioContext = new AudioContext();

    // Create audio source and analyzer node
    const audioSourceNode = audioContext.createMediaElementSource(generatePhaseAudioElement);
    audioAnalyzerNode = audioContext.createAnalyser();

    audioAnalyzerNode.fftSize = 2048;
    audioFrequencyDataArray = new Uint8Array(audioAnalyzerNode.frequencyBinCount);

    // Connect nodes
    audioSourceNode.connect(audioAnalyzerNode);
    audioAnalyzerNode.connect(audioContext.destination);

    // Clear any previous rows and state
    rowsContainerElement.innerHTML = '';
    spectrogramCanvasRows.length = 0;
    cursorOverlayCanvasRows.length = 0;
    rowStartTimeStamps.length = 0;
    annotationCanvasRows.length = 0;

    // Create the first spectrogram row to start drawing into
    createSpectrogramRow();

    // Play audio and start animation loop
    generatePhaseAudioElement.play();
    hasSpectrogramCompleted = false;
    isGeneratePhasePaused = false;
    animationFrameRequestId = requestAnimationFrame(drawSpectrogramFrame);
  }

    
</script>
  

</body>
</html>

